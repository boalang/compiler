Program(name, staticDeclarations, staticStatements, statements, combineTables, reduceTables, splitsize, seed, isLocal, outputVariableNames) ::= <<
package boa;

public class <name> extends boa.runtime.BoaRunner {
	/** {@inheritDoc} */
	@Override
	public org.apache.hadoop.mapreduce.Job job(final org.apache.hadoop.fs.Path[] ins, final org.apache.hadoop.fs.Path out) throws java.io.IOException {
		final org.apache.hadoop.mapreduce.Job job = super.job(ins, out);

		job.setJobName("<name>: " + out);

		job.setJarByClass(<name>BoaMapper.class);

		job.setMapperClass(<name>BoaMapper.class);
		<if(combineTables)>
		job.setCombinerClass(<name>BoaCombiner.class);
		<endif>
		<if(isLocal)>
		job.getConfiguration().setClass("mapred.map.output.compression.codec", org.apache.hadoop.io.compress.DefaultCodec.class, org.apache.hadoop.io.compress.CompressionCodec.class);
		<endif>
		job.setReducerClass(<name>BoaReducer.class);

		return job;
	}

	public static void main(String[] args) throws Exception {
		<if(isLocal)>org.apache.hadoop.util.ToolRunner.run(new <name>(), args);
		<else> System.exit(org.apache.hadoop.util.ToolRunner.run(new <name>(), args));
		<endif>
	}

	@Override
	public int run(String[] args) throws Exception {
		final org.apache.commons.cli.CommandLine line = parseArgs(args, getUsage());
		args = line.getArgs();
		if (args.length != 2) {
			System.err.println("Not enough arguments. Must give input directory and output directory.");
			printHelp(getUsage());
		}

		final int id;
		if (line.hasOption("job"))
			id = Integer.parseInt(line.getOptionValue("job"));
		else
			id = 0;

		final org.apache.hadoop.fs.Path[] ins = new org.apache.hadoop.fs.Path[1];
		ins[0] = new org.apache.hadoop.fs.Path(args[0] + "/projects.seq");

		final org.apache.hadoop.mapreduce.Job jb = job(ins, new org.apache.hadoop.fs.Path(args[1]));

		org.apache.hadoop.conf.Configuration configuration = jb.getConfiguration();

		configuration.set("boa.input.dir", args[0]);
		if (line.hasOption("ast"))
			configuration.set("boa.ast.dir", line.getOptionValue("ast"));
		if (line.hasOption("comments"))
			configuration.set("boa.comments.dir", line.getOptionValue("comments"));

		if (line.hasOption("splitsize"))
			configuration.setInt("mapred.max.split.size", Integer.parseInt(line.getOptionValue("splitsize")));
		else
			configuration.setInt("mapred.max.split.size", <splitsize>);

		if (line.hasOption("profile")) {
			configuration.setBoolean("mapred.task.profile", true);
			configuration.set("mapred.task.profile.maps", "1");
			configuration.set("mapred.task.profile.reduces", "0");
			//configuration.set("mapred.task.profile.params", "-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s");
			configuration.set("mapred.task.profile.params", "-agentlib:hprof=cpu=times,heap=sites,force=n,verbose=n,file=%s");
		}

		jb.setInputFormatClass(org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat.class);

		jb.setNumReduceTasks(<length(outputVariableNames)>);

		if (id > 0)
			configuration.setInt("boa.hadoop.jobid", id);

		// pass any arguments to map/reduce classes via configuration
		if (line.hasOption("excludelist"))
			configuration.setStrings("boa.exclude.projects", line.getOptionValue("excludelist"));
		if (line.hasOption("time")) {
			configuration.setBoolean("boa.debug.timings", true);
			configuration.setLong("mapred.job.reuse.jvm.num.tasks", 1);
		}

		jb.submit();


		if (id > 0)
			boa.io.BoaOutputCommitter.setJobID(jb.getJobID().toString(), id);
		System.err.println("Job ID: " + jb.getJobID().toString());

		if (line.hasOption("block")) {
			double lastSetup = -1;
			double lastMap = -1;
			double lastReduce = -1;
			while (!jb.isComplete()) {
				final double newSetup = jb.setupProgress();
				final double newMap = jb.mapProgress();
				final double newReduce = jb.reduceProgress();
				if (newSetup != lastSetup) {
					lastSetup = newSetup;
					System.err.println("SETUP : " + (newSetup * 100) + "%");
				}
				if (newMap != lastMap) {
					lastMap = newMap;
					System.err.println("MAP   : " + (newMap * 100) + "%");
				}
				if (newReduce != lastReduce) {
					lastReduce = newReduce;
					System.err.println("REDUCE: " + (newReduce * 100) + "%");
				}
				try {
					Thread.sleep (500);
				} catch (final Exception e) {}
			}
			System.err.println("JOB FINISHED: " + (jb.isSuccessful() ? "Success" : "Failed"));
			return jb.isSuccessful() ? 0 : 1;
		}
		return 0;
	}

	public String getUsage() {
		return "\<inputDir> \<outputDir>";
	}

	static class <name>BoaMapper extends boa.runtime.BoaMapper {
		<SetVarNames(outputVariableNames)>
		<staticDeclarations>
		<if(staticStatements)>
		{
			<staticStatements>
		}

		<endif>
		/** {@inheritDoc} */
		@Override
		protected void map(final org.apache.hadoop.io.Text key, final org.apache.hadoop.io.BytesWritable value, final org.apache.hadoop.mapreduce.Mapper\<org.apache.hadoop.io.Text, org.apache.hadoop.io.BytesWritable, boa.io.EmitKey, boa.io.EmitValue>.Context context) throws java.io.IOException {
			if (excludeProject(key.toString())) {
				LOG.info("EXCLUDED PROJECT: " + key.toString());
				return;
			}

			if (context.getConfiguration().getBoolean("boa.debug.timings", false))
				LOG.info(key.toString());

			try {
				boa.functions.BoaMathIntrinsics.random = new java.util.Random(<seed> + key.hashCode());
				boa.types.Toplevel.Project _input = boa.types.Toplevel.Project.parseFrom(com.google.protobuf.CodedInputStream.newInstance(value.getBytes(), 0, value.getLength()));
				<statements:{s | <s><\n>}>
			} catch (final Throwable e) {
				LOG.error(e.getClass().getName() + " caught", e);
				boa.io.BoaOutputCommitter.lastSeenEx = e;
				throw new java.io.IOException("map failure for key '" + key.toString() + "'", e);
			}

			if (context.getConfiguration().getBoolean("boa.debug.timings", false))
				LOG.info(key.toString());
		}

		/** {@inheritDoc} */
		@Override
		protected void setup(final org.apache.hadoop.mapreduce.Mapper.Context context) throws java.io.IOException, java.lang.InterruptedException {
			boa.functions.BoaAstIntrinsics.setup(context);
			super.setup(context);
		}

		/** {@inheritDoc} */
		@Override
		protected void cleanup(final org.apache.hadoop.mapreduce.Mapper.Context context) throws java.io.IOException, java.lang.InterruptedException {
			boa.functions.BoaAstIntrinsics.cleanup(context);
			super.cleanup(context);
		}
	}

	static class <name>BoaCombiner extends boa.runtime.BoaCombiner {
		<SetVarNames(outputVariableNames)>
		public <name>BoaCombiner() {
			super();

			<combineTables:{t | <t><\n>}>		}
	}

	static class <name>BoaReducer extends boa.runtime.BoaReducer {
		<SetVarNames(outputVariableNames)>
		public <name>BoaReducer() {
			super();

			<reduceTables:{t | <t><\n>}>		}
	}

	@Override
	public org.apache.hadoop.mapreduce.Mapper getMapper() {
		return new <name>BoaMapper();
	}

	@Override
	public boa.runtime.BoaCombiner getCombiner() {
		return new <name>BoaCombiner();
	}

	@Override
	public boa.runtime.BoaReducer getReducer() {
		return new <name>BoaReducer();
	}
}
>>

SetVarNames(vars) ::= <<
static {
	boa.runtime.BoaPartitioner.setVariableNames(new String[] {<vars:{v|<v>}; separator=", ">});
}
>>

EmitStatement(indices, id, expression, weight) ::= "context.write(new boa.io.EmitKey(<if(indices)><indices:{idx | \"[\" + (<idx>) + \"]\"}; separator=\" + \">, <endif><id>), new boa.io.EmitValue(<expression><if(weight)>, <weight><endif>));<\n>"
